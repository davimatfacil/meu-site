---
title: "Quanto Vale uma Casa? A Estat√≠stica que Responde"
author: "prof. Davi Rocha"
date: "2026-02-19"
categories: [R, Estat√≠stica, Regress√£o, Data Science]
execute:
  warning: false
  message: false
---

```{r}
#| include: false
library(dplyr)
library(ggplot2)
library(corrplot)

dados <- read.csv('precos_de_casas.csv')
dados <- dados %>% select(-Id)
```

## O problema que toda imobili√°ria tem

Imagine que voc√™ √© analista de dados e recebe o seguinte desafio: *"Preciso precificar centenas de im√≥veis. Como fa√ßo isso?"*

A resposta ing√™nua, ou n√£o t√£o ing√™nua assim, dependendo do seu  repert√≥rio seria: "olha o pre√ßo dos vizinhos". 
Curiosidade: existe um algoritmo famoso chamado KNN (*K-Nearest Neighbors*) que faz 
literalmente isso. Mas hoje vamos por outro caminho.

Como cientista de dados, podemos construir um **modelo matem√°tico**  que, a partir de dados hist√≥ricos, **estima** quais caracter√≠sticas de um im√≥vel determinam o seu pre√ßo e como cada uma delas se associa √† varia√ß√£o do pre√ßo. Repare que escrevi *estima*, n√£o *aprende*. Modelos n√£o aprendem. Eles ajustam par√¢metros para minimizar erros. Essa distin√ß√£o parece sutil, mas faz toda a diferen√ßa quando voc√™ precisa interpretar e questionar os resultados.

O dataset √© inspirado na competi√ß√£o [House Prices do Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques){target="_blank"}, simplificado para fins did√°ticos. Trabalharemos com as seguintes vari√°veis:

| Vari√°vel | Descri√ß√£o |
|---|---|
| `area_primeiro_andar` | √Årea √∫til do andar t√©rreo (m¬≤) |
| `area_segundo_andar` | √Årea √∫til do segundo andar (m¬≤) |
| `quantidade_banheiros` | N√∫mero de banheiros |
| `capacidade_carros_garagem` | Capacidade da garagem (m¬≤) |
| `qualidade_da_cozinha_Excelente` | 1 = cozinha de alto padr√£o, 0 = demais |
| `preco_de_venda` | üéØ Vari√°vel alvo |

---

## 1. Conhecendo os dados

Antes de qualquer modelo, precisamos enxergar o que temos. As primeiras linhas do dataset j√° revelam muito:

```{r}
#| echo: false
knitr::kable(
  head(dados, 6),
  format = "html",
  digits = 2,
  col.names = c("1¬∫ Andar (m¬≤)", "2¬∫ Andar?", "2¬∫ Andar (m¬≤)",
                "Banheiros", "Garagem (m¬≤)", "Cozinha Excel.", "Pre√ßo (R$)"),
  caption = "Primeiras linhas do dataset"
)
```

O dataset tem `r nrow(dados)` im√≥veis e `r ncol(dados)` vari√°veis. Sem valores ausentes, o que nos poupa da etapa de limpeza.



---

## 2. Existe rela√ß√£o entre √°rea e pre√ßo?

Antes de calcular qualquer coisa, precisamos **ver** o que est√° acontecendo. O gr√°fico de dispers√£o geralmente √© o primeiro passo.

```{r}
#| echo: false
#| fig-align: center
#| fig-cap: "Cada ponto √© um im√≥vel. A nuvem sobe da esquerda para a direita ‚Äî sinal claro de tend√™ncia linear."
ggplot(data = dados, aes(x = area_primeiro_andar, y = preco_de_venda)) +
  geom_point(alpha = 0.6, color = "#2c7bb6", size = 2) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Pre√ßo √ó √Årea do Primeiro Andar",
    x = "√Årea do primeiro andar (m¬≤)",
    y = "Pre√ßo de venda (R$)"
  ) +
  theme_minimal(base_size = 13)
```

A "nuvem" de pontos sobe da esquerda para a direita. Im√≥veis maiores tendem a custar mais. Mas ser√° que √© uma rela√ß√£o forte?

---

## 3. Medindo a for√ßa da rela√ß√£o: o coeficiente de correla√ß√£o

O **coeficiente de correla√ß√£o de Pearson** (*r*) quantifica a for√ßa e a dire√ß√£o de uma rela√ß√£o linear entre duas vari√°veis. Ele varia entre ‚àí1 e 1:

- **r = 1**: correla√ß√£o positiva perfeita
- **r = 0**: sem rela√ß√£o linear
- **r = ‚àí1**: correla√ß√£o negativa perfeita

```{r}
#| echo: false
r <- cor(dados$area_primeiro_andar, dados$preco_de_venda)
```

No nosso dataset, a correla√ß√£o entre √°rea e pre√ßo √© **r = `r round(r, 2)`** , uma correla√ß√£o positiva moderada a forte. A √°rea √© um bom preditor, mas n√£o √© o √∫nico fator que interessa.

---

## 4. Encontrando a melhor reta: regress√£o linear simples

Olhando o gr√°fico, √© possivel tra√ßar "uma" reta passando por eles(volte ao gr√°fico e veja!). Mas qual √© a **melhor** reta?  Da estat√≠stica, sabemos que esse reta deve minimizar a diferen√ßa entre os valores previstos e os valores reais. Essas diferen√ßas se chamam **res√≠duos**.

A fun√ß√£o de regress√£o √©:

$$\hat{Y} = \beta_0 + \beta_1 X$$

Onde $\hat{Y}$ √© o pre√ßo estimado, $X$ √© a √°rea, $\beta_0$ √© o intercepto e $\beta_1$ √© o quanto o pre√ßo sobe a cada 1 m¬≤ adicional.

```{r}
#| echo: false
dados_rl <- dados %>% select(area_primeiro_andar, preco_de_venda)
modelo <- lm(preco_de_venda ~ area_primeiro_andar, data = dados_rl)
b0 <- round(coef(modelo)[1], 0)
b1 <- round(coef(modelo)[2], 0)
```

O R encontra automaticamente os coeficientes pelo m√©todo dos m√≠nimos quadrados. O resultado do nosso modelo:

$$\text{pre√ßo} = `r format(b0, big.mark=".")` + `r format(b1, big.mark=".")` \times \text{√°rea}$$

Ou seja: **cada metro quadrado adicional agrega, em m√©dia, R$ `r format(b1, big.mark=".")` ao pre√ßo do im√≥vel**.

```{r}
#| echo: false
#| fig-align: center
#| fig-cap: "A reta vermelha √© a reta de regress√£o ‚Äî a que minimiza a soma dos quadrados dos res√≠duos."
ggplot(data = dados_rl, aes(x = area_primeiro_andar, y = preco_de_venda)) +
  geom_point(alpha = 0.6, color = "#2c7bb6", size = 2) +
  geom_abline(intercept = coef(modelo)[1], slope = coef(modelo)[2],
              color = "red", linewidth = 1.2) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Regress√£o Linear: Pre√ßo √ó √Årea",
    x = "√Årea do primeiro andar (m¬≤)",
    y = "Pre√ßo de venda (R$)"
  ) +
  theme_minimal(base_size = 13)
```

---

## 5. O modelo √© bom? M√©tricas e testes

### R¬≤:  quanto o modelo explica?

```{r}
#| echo: false
r2 <- round(summary(modelo)$r.squared, 2)
```

O **R¬≤** mede a propor√ß√£o da varia√ß√£o do pre√ßo que o modelo explica. No nosso modelo simples, R¬≤ = **`r r2`** , ou seja, a √°rea sozinha explica **`r r2*100`%** da varia√ß√£o dos pre√ßos. √â um come√ßo promissor, mas h√° muito mais para avaliar.

### Teste t ‚Äî a vari√°vel realmente importa?

O teste t verifica se o coeficiente da √°rea √© estatisticamente diferente de zero:

- **H‚ÇÄ**: a √°rea n√£o afeta o pre√ßo (coeficiente = 0)
- **H‚ÇÅ**: a √°rea afeta o pre√ßo (coeficiente ‚â† 0)

```{r}
#| echo: false
p_val <- coef(summary(modelo))["area_primeiro_andar", "Pr(>|t|)"]
```

O p-valor obtido √© `r format(p_val, scientific = TRUE, digits = 2)` ‚Äî essencialmente zero. **Rejeitamos H‚ÇÄ com seguran√ßa**: a √°rea tem efeito real e significativo sobre o pre√ßo.

### An√°lise de res√≠duos ‚Äî o modelo est√° bem calibrado?

Todo modelo de regress√£o tem duas premissas b√°sicas sobre os res√≠duos:

1. **M√©dia igual a zero**: os erros n√£o t√™m vi√©s sistem√°tico
2. **Homocedasticidade**: a vari√¢ncia dos erros √© constante

```{r}
#| echo: false
dados_rl$previsao <- predict(modelo, newdata = dados_rl)
dados_rl$residuo  <- resid(modelo)
media_res <- round(mean(dados_rl$residuo), 4)
```

A m√©dia dos res√≠duos √© **`r media_res`** , ou seja, zero. ‚úÖ

Mas observe o gr√°fico abaixo. Ele conta uma hist√≥ria importante. 

```{r}
#| echo: false
#| fig-align: center
#| fig-cap: "Observe a abertura da nuvem conforme o pre√ßo estimado cresce ‚Äî um padr√£o de funil cl√°ssico. O modelo √© preciso para im√≥veis baratos e impreciso para im√≥veis de alto valor."
ggplot(data = dados_rl, aes(x = previsao, y = residuo)) +
  geom_point(alpha = 0.6, color = "#2c7bb6", size = 2) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = "Res√≠duos √ó Previs√£o",
    x = "Pre√ßo Estimado (R$)",
    y = "Res√≠duo (R$)"
  ) +
  theme_minimal(base_size = 13)
```

Os pontos parecem estar se afastando da linha pontilhada vermelha conforme o pre√ßo aumenta.
Depois voltamos a isso.

---

## 6. Indo al√©m: regress√£o m√∫ltipla

A √°rea explica `r r2*100`% dos pre√ßos. E o restante? Outras caracter√≠sticas do im√≥vel est√£o escondidas a√≠. A regress√£o m√∫ltipla permite incorporar todas elas simultaneamente.

Primeiro, um mapa de calor das correla√ß√µes para identificar as vari√°veis mais promissoras:

```{r}
#| echo: false
#| fig-align: center
#| fig-cap: "Quanto mais vermelho, maior a correla√ß√£o positiva com o pre√ßo de venda."
correlacao <- cor(dados[, c(
  'area_primeiro_andar', 'area_segundo_andar',
  'quantidade_banheiros', 'capacidade_carros_garagem',
  'qualidade_da_cozinha_Excelente', 'preco_de_venda'
)])

corrplot(correlacao, method = "color", type = "upper",
         col = colorRampPalette(c("blue", "white", "red"))(200),
         tl.cex = 0.8, tl.col = "black",
         addCoef.col = "black", number.cex = 0.7)
```

Com o mapa de calor em m√£os, constru√≠mos modelos progressivamente mais ricos e comparamos o **RSE** (Erro Padr√£o Residual) de cada um, lembre-se que quanto menor, melhor:

```{r}
#| echo: false
modelo_1 <- lm(preco_de_venda ~ area_primeiro_andar + area_segundo_andar, data = dados)
modelo_2 <- lm(preco_de_venda ~ area_primeiro_andar + area_segundo_andar + quantidade_banheiros, data = dados)
modelo_3 <- lm(preco_de_venda ~ area_primeiro_andar + area_segundo_andar + quantidade_banheiros + capacidade_carros_garagem, data = dados)
modelo_multi <- lm(preco_de_venda ~ area_primeiro_andar + area_segundo_andar + quantidade_banheiros + capacidade_carros_garagem + qualidade_da_cozinha_Excelente, data = dados)

comparacao <- data.frame(
  Modelo = c("Modelo 1 (+ 2¬∫ andar)", "Modelo 2 (+ banheiros)",
             "Modelo 3 (+ garagem)", "Modelo Final (+ cozinha)"),
  Variaveis = c(2, 3, 4, 5),
  R2 = round(c(summary(modelo_1)$r.squared, summary(modelo_2)$r.squared,
               summary(modelo_3)$r.squared, summary(modelo_multi)$r.squared), 3),
  RSE = round(c(summary(modelo_1)$sigma, summary(modelo_2)$sigma,
                summary(modelo_3)$sigma, summary(modelo_multi)$sigma), 0)
)

knitr::kable(comparacao, format = "html",
             col.names = c("Modelo", "Vari√°veis", "R¬≤", "RSE (R$)"),
             caption = "Compara√ß√£o dos modelos ‚Äî cada vari√°vel adicionada reduz o erro")
```

O modelo final, com 5 vari√°veis, apresenta o **maior R¬≤** e o **menor RSE**. √â o mais preciso do conjunto.

### A equa√ß√£o do modelo final

Assim como fizemos na regress√£o simples, podemos escrever a equa√ß√£o completa com os coeficientes reais:

```{r}
#| echo: false
cf <- round(coef(modelo_multi), 0)
```

$$\hat{Y} = `r format(cf[1], big.mark=".")` + `r format(cf[2], big.mark=".")` \cdot X_{\text{1¬∫ andar}} + `r format(cf[3], big.mark=".")` \cdot X_{\text{2¬∫ andar}} + `r format(cf[4], big.mark=".")` \cdot X_{\text{banheiros}} + `r format(cf[5], big.mark=".")` \cdot X_{\text{garagem}} + `r format(cf[6], big.mark=".")` \cdot X_{\text{cozinha}}$$

Lendo a equa√ß√£o:

- Cada m¬≤ do **1¬∫ andar** agrega R$ `r format(cf[2], big.mark=".")` ao pre√ßo
- Cada m¬≤ do **2¬∫ andar** agrega R$ `r format(cf[3], big.mark=".")` ao pre√ßo
- Cada **banheiro** adicional agrega R$ `r format(cf[4], big.mark=".")` ao pre√ßo
- Cada m¬≤ de **garagem** agrega R$ `r format(cf[5], big.mark=".")` ao pre√ßo
- Uma **cozinha excelente** agrega R$ `r format(cf[6], big.mark=".")` ao pre√ßo ‚Äî mantendo tudo mais constante

---

## 7. Validando o modelo final: teste F

Na regress√£o simples usamos o teste *t* para uma vari√°vel. Na regress√£o m√∫ltipla, o **teste F** valida o conjunto todo:

- **H‚ÇÄ**: nenhuma vari√°vel afeta o pre√ßo
- **H‚ÇÅ**: pelo menos uma vari√°vel afeta o pre√ßo

```{r}
#| echo: false
estat_F   <- summary(modelo_multi)$fstatistic
p_valor_F <- pf(estat_F[1], estat_F[2], estat_F[3], lower.tail = FALSE)
```

O p-valor do teste F √© essencialmente zero. 
**O conjunto de vari√°veis tem poder explicativo real sobre o pre√ßo.** ‚úÖ

---

## 8. Prevendo pre√ßos de im√≥veis novos

Agora a parte mais concreta: usar o modelo para precificar im√≥veis que ele nunca viu.

```{r}
#| echo: false
casas_novas <- read.csv('novas_casas_extra.csv', sep = ';')
casas_novas$previsao <- predict(modelo_multi, newdata = casas_novas)

knitr::kable(
  casas_novas %>% select(-Casa) %>%
    mutate(previsao = round(previsao, 0)),
  format = "html",
  digits = 2,
  col.names = c("1¬∫ Andar (m¬≤)", "2¬∫ Andar?", "2¬∫ Andar (m¬≤)",
                "Banheiros", "Garagem (m¬≤)", "Cozinha Excel.", "Pre√ßo Estimado (R$)"),
  caption = "Previs√£o de pre√ßos para im√≥veis novos"
)
```

---

## O que aprendemos

Partimos de uma pergunta simples ‚Äî *quanto vale uma casa?* ‚Äî e chegamos a um pipeline completo:

1. **Explora√ß√£o visual** para verificar linearidade
2. **Correla√ß√£o de Pearson** para medir for√ßa das rela√ß√µes
3. **Regress√£o simples** para encontrar a melhor reta
4. **M√©tricas** (R¬≤, RSE) e testes (t, F) para validar o modelo
5. **An√°lise de res√≠duos** para checar premissas
6. **Regress√£o m√∫ltipla** para incorporar mais vari√°veis
7. **Previs√£o** de novos valores

Regress√£o linear n√£o √© apenas uma f√≥rmula. √â um processo de perguntas e respostas: o que influencia o pre√ßo? Com que for√ßa? O modelo est√° errando de forma sistem√°tica? Essas perguntas guiam o cientista de dados em dire√ß√£o a modelos cada vez mais honestos e √∫teis.

---

## Limita√ß√µes ‚Äî o modelo tem problemas, e tudo bem

Eu poderia encerrar o artigo no ponto anterior e fingir que est√° tudo certo. Mas n√£o estaria sendo honesto com voc√™.

Volte ao gr√°fico de res√≠duos da regress√£o simples. Observe com calma o que acontece da esquerda para a direita:

- Para **im√≥veis baratos** (pre√ßo estimado entre 400k e 700k): os res√≠duos ficam concentrados em torno de zero. O modelo acerta bem.
- Para **im√≥veis de alto valor** (acima de 1 milh√£o): a dispers√£o explode. Os erros chegam a ¬±1.000.000. O modelo se perde.

Isso tem nome: **heterocedasticidade**. O padr√£o de funil cl√°ssico. A vari√¢ncia dos erros n√£o √© constante ‚Äî ela cresce junto com o valor do im√≥vel.

Por que isso acontece? Pense bem. Im√≥veis baratos s√£o relativamente previs√≠veis: √°rea e n√∫mero de banheiros j√° explicam boa parte do pre√ßo. Im√≥veis de luxo s√£o outra hist√≥ria. Acabamento, localiza√ß√£o exata, arquitetura, vista, hist√≥ria do im√≥vel, caracter√≠sticas que nosso modelo simplesmente n√£o enxerga. A incerteza cresce porque o modelo n√£o tem as informa√ß√µes certas para esse segmento.

**O modelo √© bom para im√≥veis comuns. Para im√≥veis de luxo, ele chuta.**

O que poderia melhorar?

- **Transforma√ß√£o logar√≠tmica**: aplicar `log(preco_de_venda)` como vari√°vel dependente. Em dados de pre√ßo, erros percentuais costumam ser mais est√°veis que erros absolutos. √â a solu√ß√£o mais cl√°ssica para esse padr√£o.
- **Mais vari√°veis de qualidade**: acabamento, localiza√ß√£o, idade do im√≥vel. O modelo atual n√£o tem nada que diferencie um apartamento simples de um im√≥vel de alto padr√£o al√©m da cozinha.
- **Modelos n√£o-lineares**: Se o foco for precis√£o nas previs√µes, Random Forest ou Gradient Boosting capturam rela√ß√µes mais complexas que a reta n√£o consegue.

Mas sabe o que √© mais importante do que qualquer uma dessas solu√ß√µes t√©cnicas? **Saber identificar o problema.** Um cientista de dados que entrega um modelo de regress√£o linear sem olhar o gr√°fico de res√≠duos √© como um m√©dico que prescreve o rem√©dio sem ler o resultado do exame.

Isso √© o que separa um cientista de dados de um apertador de bot√µes.

---


